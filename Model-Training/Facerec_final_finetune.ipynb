{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgleTxp1z93-",
        "outputId": "7fa1f219-96f2-4d85-e128-5bb4b432a0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting deepface\n",
            "  Downloading deepface-0.0.92-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.0.3)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.1.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.66.4)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (9.4.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.8.0.76)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.15.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.5)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.9.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.4.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2024.6.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.43.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=1.9.0->deepface) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=c9dcb8567fd1748506430184eb2e8714d14488110af339d307cdc36ef2204b28\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: typeguard, smmap, setproctitle, sentry-sdk, gunicorn, fire, docker-pycreds, tensorflow-addons, mtcnn, gitdb, gitpython, wandb, retina-face, deepface\n",
            "Successfully installed deepface-0.0.92 docker-pycreds-0.4.0 fire-0.6.0 gitdb-4.0.11 gitpython-3.1.43 gunicorn-22.0.0 mtcnn-0.1.1 retina-face-0.0.17 sentry-sdk-2.9.0 setproctitle-1.3.3 smmap-5.0.1 tensorflow-addons-0.23.0 typeguard-2.13.3 wandb-0.17.4\n"
          ]
        }
      ],
      "source": [
        "!pip install deepface wandb tensorflow-addons keras huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpInCToe_HIt"
      },
      "source": [
        "# Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwSXiTQj_Cmw",
        "outputId": "a871e42a-b661-49b0-8160-aa3d50cd7bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-07-10 12:09:54--  http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
            "Resolving vis-www.cs.umass.edu (vis-www.cs.umass.edu)... 128.119.244.95\n",
            "Connecting to vis-www.cs.umass.edu (vis-www.cs.umass.edu)|128.119.244.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180566744 (172M) [application/x-gzip]\n",
            "Saving to: ‘lfw.tgz’\n",
            "\n",
            "lfw.tgz             100%[===================>] 172.20M  2.43MB/s    in 2m 13s  \n",
            "\n",
            "2024-07-10 12:12:07 (1.30 MB/s) - ‘lfw.tgz’ saved [180566744/180566744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zyL_5Zu_bxp"
      },
      "outputs": [],
      "source": [
        "!tar -xzvf lfw.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM40wNF2AGlM"
      },
      "source": [
        "write code to clean the dataset where if any folder in lfw/ has less than 4 images then delete the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S664WK2a_bvB",
        "outputId": "1e7c0770-7741-41c9-9224-4d28792e607a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 3 folders\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "count=0\n",
        "for folder in os.listdir('lfw'):\n",
        "    if len(os.listdir(f'lfw/{folder}')) < 5:\n",
        "        shutil.rmtree(f'lfw/{folder}')\n",
        "        count+=1\n",
        "print(f\"Removed {count} folders\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQw5KKTCjw_m",
        "outputId": "e7b9cb7c-2524-4164-dd81-9e26dfca89c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(len(os.listdir('lfw')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQD62KaBBD_A"
      },
      "source": [
        "Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw8Wf3GsAPAN"
      },
      "outputs": [],
      "source": [
        "import deepface\n",
        "from deepface.basemodels.Facenet import load_facenet512d_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WYL7RIDlDM"
      },
      "outputs": [],
      "source": [
        "def get_embedding_module(imageSize):\n",
        "\n",
        "    # Load Facenet Model\n",
        "    model = load_facenet512d_model()\n",
        "\n",
        "    inputs = keras.Input(shape=(imageSize, imageSize, 3))\n",
        "\n",
        "    # Resizing the input\n",
        "    x = layers.Resizing(160, 160)(inputs)\n",
        "    output = model(x)\n",
        "    embedding = keras.Model(inputs, output, name=\"embedding\")\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT6pIJrCGZtP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "directory = '/root/.deepface/weights'\n",
        "os.makedirs(directory, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrh_a7B7Cjth"
      },
      "outputs": [],
      "source": [
        "def get_siamese_network(imageSize, embeddingModel):\n",
        "    # build the anchor, positive and negative input layer\n",
        "    anchorInput = keras.Input(name=\"anchor\", shape=imageSize + (3,))\n",
        "    positiveInput = keras.Input(name=\"positive\", shape=imageSize + (3,))\n",
        "    negativeInput = keras.Input(name=\"negative\", shape=imageSize + (3,))\n",
        "    # embed the anchor, positive and negative images\n",
        "    anchorEmbedding = embeddingModel(anchorInput)\n",
        "    positiveEmbedding = embeddingModel(positiveInput)\n",
        "    negativeEmbedding = embeddingModel(negativeInput)\n",
        "    # build the siamese network and return it\n",
        "    siamese_network = keras.Model(\n",
        "        inputs=[anchorInput, positiveInput, negativeInput],\n",
        "        outputs=[anchorEmbedding, positiveEmbedding, negativeEmbedding]\n",
        "    )\n",
        "    return siamese_network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN9GRVlryiwi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Mean\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azk1ayQiyhQL"
      },
      "outputs": [],
      "source": [
        "class TripletDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size, image_size, num_classes):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.num_classes = num_classes\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.encoded_labels = self.label_encoder.fit_transform(labels)\n",
        "        self.image_data_generator = ImageDataGenerator()\n",
        "        self.on_epoch_end()\n",
        "        print(f\"Initialized TripletDataGenerator with {len(self.image_paths)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(1, len(self.image_paths) // self.batch_size)  # Ensure at least one batch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_labels = self.encoded_labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        return self._generate_triplet_batch(batch_image_paths, batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle the data at the end of each epoch\n",
        "        combined = list(zip(self.image_paths, self.encoded_labels))\n",
        "        np.random.shuffle(combined)\n",
        "        self.image_paths[:], self.encoded_labels[:] = zip(*combined)\n",
        "\n",
        "    def _generate_triplet_batch(self, batch_image_paths, batch_labels):\n",
        "        anchor_images = []\n",
        "        positive_images = []\n",
        "        negative_images = []\n",
        "\n",
        "        for i in range(len(batch_image_paths)):\n",
        "            anchor_path = batch_image_paths[i]\n",
        "            anchor_label = batch_labels[i]\n",
        "\n",
        "            positive_path = np.random.choice(\n",
        "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l == anchor_label]\n",
        "            )\n",
        "            negative_path = np.random.choice(\n",
        "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l != anchor_label]\n",
        "            )\n",
        "\n",
        "            anchor_image = load_img(anchor_path, target_size=self.image_size)\n",
        "            positive_image = load_img(positive_path, target_size=self.image_size)\n",
        "            negative_image = load_img(negative_path, target_size=self.image_size)\n",
        "\n",
        "            anchor_images.append(img_to_array(anchor_image))\n",
        "            positive_images.append(img_to_array(positive_image))\n",
        "            negative_images.append(img_to_array(negative_image))\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"anchor\": np.array(anchor_images),\n",
        "                \"positive\": np.array(positive_images),\n",
        "                \"negative\": np.array(negative_images)\n",
        "            },\n",
        "            None,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZHmg8U3Cjie"
      },
      "outputs": [],
      "source": [
        "class SiameseModel(keras.Model):\n",
        "    def __init__(self, siameseNetwork, margin, lossTracker):\n",
        "        super().__init__()\n",
        "        self.siameseNetwork = siameseNetwork\n",
        "        self.margin = margin\n",
        "        self.lossTracker = lossTracker\n",
        "\n",
        "    def _compute_distance(self, inputs):\n",
        "        anchor, positive, negative = inputs[\"anchor\"], inputs[\"positive\"], inputs[\"negative\"]\n",
        "        # embed the images using the siamese network\n",
        "        embeddings = self.siameseNetwork((anchor, positive, negative))\n",
        "        anchorEmbedding = embeddings[0]\n",
        "        positiveEmbedding = embeddings[1]\n",
        "        negativeEmbedding = embeddings[2]\n",
        "        # calculate the anchor to positive and negative distance\n",
        "        apDistance = tf.reduce_sum(\n",
        "            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n",
        "        )\n",
        "        anDistance = tf.reduce_sum(\n",
        "            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n",
        "        )\n",
        "        # return the distances\n",
        "        return (apDistance, anDistance)\n",
        "\n",
        "    def _compute_loss(self, apDistance, anDistance):\n",
        "        loss = apDistance - anDistance\n",
        "        loss = tf.maximum(loss + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # compute the distance between the anchor and positive,\n",
        "        # negative images\n",
        "        apDistance, anDistance = self._compute_distance(inputs)\n",
        "        return (apDistance, anDistance)\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # compute the distance between the anchor and positive,\n",
        "            # negative images\n",
        "            apDistance, anDistance = self._compute_distance(inputs)\n",
        "            # calculate the loss of the siamese network\n",
        "            loss = self._compute_loss(apDistance, anDistance)\n",
        "        # compute the gradients and optimize the model\n",
        "        gradients = tape.gradient(\n",
        "            loss,\n",
        "            self.siameseNetwork.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.siameseNetwork.trainable_variables)\n",
        "        )\n",
        "        # update the metrics and return the loss\n",
        "        self.lossTracker.update_state(loss)\n",
        "        return {\"loss\": self.lossTracker.result()}\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        # compute the distance between the anchor and positive,\n",
        "        # negative images\n",
        "        apDistance, anDistance = self._compute_distance(inputs)\n",
        "        # calculate the loss of the siamese network\n",
        "        loss = self._compute_loss(apDistance, anDistance)\n",
        "        # update the metrics and return the loss\n",
        "        self.lossTracker.update_state(loss)\n",
        "        return {\"loss\": self.lossTracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.lossTracker]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"siameseNetwork\": self.siameseNetwork,\n",
        "            \"margin\": self.margin,\n",
        "            \"lossTracker\": self.lossTracker\n",
        "            })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUQTnJIM0Pdb"
      },
      "source": [
        "# Try-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqQzNCGv0SNs"
      },
      "outputs": [],
      "source": [
        "# Set the directory structure\n",
        "data_dir = 'lfw'\n",
        "image_size = (250, 250)\n",
        "batch_size = 32  # Adjust the batch size for the small dataset\n",
        "margin = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "75hktjyX0UBD",
        "outputId": "f205897f-7cf2-4825-acab-6fee2cc646bb"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240710_121438-il25peeq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shah-devansh-11/FaceRec/runs/il25peeq' target=\"_blank\">helpful-fog-2</a></strong> to <a href='https://wandb.ai/shah-devansh-11/FaceRec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shah-devansh-11/FaceRec' target=\"_blank\">https://wandb.ai/shah-devansh-11/FaceRec</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shah-devansh-11/FaceRec/runs/il25peeq' target=\"_blank\">https://wandb.ai/shah-devansh-11/FaceRec/runs/il25peeq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shah-devansh-11/FaceRec/runs/il25peeq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7b348053f340>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize W&B\n",
        "wandb.init(project=\"FaceRec\", config={\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"epochs\": 20,\n",
        "    \"batch_size\": 32,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"architecture\": \"FaceNet\",\n",
        "    \"dataset\": \"lfw\",\n",
        "    \"loss\": \"TripletLoss\",\n",
        "    \"margin\": 1.0\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MRdQa7x0V77"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the data\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for label in os.listdir(data_dir):\n",
        "    label_dir = os.path.join(data_dir, label)\n",
        "    if os.path.isdir(label_dir):\n",
        "        for image_name in os.listdir(label_dir):\n",
        "            image_paths.append(os.path.join(label_dir, image_name))\n",
        "            labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmqDeSlB0iFt",
        "outputId": "0688a984-5f81-4ff3-bd59-0265360c00e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1140 images in 5 classes\n",
            "Training on 912 images\n",
            "Validating on 228 images\n"
          ]
        }
      ],
      "source": [
        "# Debugging output\n",
        "print(f\"Found {len(image_paths)} images in {len(set(labels))} classes\")\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Check if the splits are non-empty\n",
        "print(f\"Training on {len(train_paths)} images\")\n",
        "print(f\"Validating on {len(val_paths)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdTteKRx0lN1",
        "outputId": "7ef1a8a4-d47f-46b0-bc0b-e45554260458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized TripletDataGenerator with 912 images\n",
            "Initialized TripletDataGenerator with 228 images\n"
          ]
        }
      ],
      "source": [
        "# Create data generators\n",
        "num_classes = len(set(labels))\n",
        "train_generator = TripletDataGenerator(train_paths, train_labels, batch_size, image_size, num_classes)\n",
        "val_generator = TripletDataGenerator(val_paths, val_labels, batch_size, image_size, num_classes)\n",
        "\n",
        "# Check if the generators have data\n",
        "assert len(train_generator) > 0, \"Training generator is empty!\"\n",
        "assert len(val_generator) > 0, \"Validation generator is empty!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyCQl9VD0qy_",
        "outputId": "e3e243ee-10e9-4496-bdb2-52a1ab27301b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24-07-10 12:14:58 - facenet512_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facenet512_weights.h5\n",
            "To: /root/.deepface/weights/facenet512_weights.h5\n",
            "100%|██████████| 95.0M/95.0M [00:00<00:00, 97.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Create the embedding model and the Siamese network\n",
        "embedding_model = get_embedding_module(image_size[0])\n",
        "siamese_network = get_siamese_network(image_size, embedding_model)\n",
        "\n",
        "# Initialize the Siamese model\n",
        "loss_tracker = Mean(name=\"loss\")\n",
        "siamese_model = SiameseModel(siamese_network, margin, loss_tracker)\n",
        "\n",
        "# Compile the model\n",
        "siamese_model.compile(optimizer=Adam())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkjZAKf6CjfQ",
        "outputId": "9680224e-f7dd-403b-90f6-53c70c894b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "114/114 [==============================] - ETA: 0s - loss: 62.7843"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "114/114 [==============================] - 747s 7s/step - loss: 62.7843 - val_loss: 1.0428\n",
            "Epoch 2/2\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0216"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 6.7s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r114/114 [==============================] - 745s 7s/step - loss: 1.0216 - val_loss: 0.9650\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b3484acceb0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "siamese_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "bnnI1Xkc1QAO",
        "outputId": "bf29c32c-6725-4704-b7b3-04e50c712c56"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'SiameseModel' object has no attribute 'push_to_hub'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e47dfe96d504>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Devansh-Shah11/FaceNet_finetuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'SiameseModel' object has no attribute 'push_to_hub'"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/models\",\n",
        "    repo_id=\"DShah-11/FaceNet_Finetuned\",\n",
        "    repo_type=\"model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8veuJ9UD2oa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
